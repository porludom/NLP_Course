{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the bigram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoQeEsZvHvvi",
        "outputId": "7cb8a591-3f35-4a0c-93b3-1af7e620fd11"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import linecache\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from math import log10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "z6J902x5ueRN"
      },
      "outputs": [],
      "source": [
        "prob_dict = {} # probability of words in different contexts\n",
        "min_prob = 1.0\n",
        "dictionary = defaultdict(lambda: 1) # frequency of words\n",
        "all_words_counter = 0 # total number of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_-1ANx0juwni"
      },
      "outputs": [],
      "source": [
        "def get_pair_from_bigrams_set():\n",
        "  with open(\"./data/bigrams.txt\", \"r\", encoding='latin-1') as file:\n",
        "    for line in file:\n",
        "      yield line.lower().split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iW0UcFAIvofc"
      },
      "outputs": [],
      "source": [
        "# TRAINING BIGRAMS\n",
        "counter_without_words = {}\n",
        "counter_with_words = {}\n",
        "for value, context, word in get_pair_from_bigrams_set():\n",
        "  value = int(value)\n",
        "  if(context in counter_without_words):\n",
        "      counter_without_words[context]+= value\n",
        "  else:\n",
        "      counter_without_words[context]= value\n",
        "\n",
        "  if((word, context) in counter_with_words):\n",
        "      counter_with_words[(word,context)]+= value\n",
        "  else:\n",
        "      counter_with_words[(word,context)]= value\n",
        "\n",
        "  dictionary[word]+=value\n",
        "  all_words_counter+=value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "d2wLDnF88U_h"
      },
      "outputs": [],
      "source": [
        "def get_pair_from_coca_set():\n",
        "  with open(\"./data/coca_all_links.txt\", \"r\", encoding='latin-1') as file:\n",
        "    for line in file:\n",
        "      yield line.lower().split()[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "s2dx6K_P80iy"
      },
      "outputs": [],
      "source": [
        "for value, context, word in get_pair_from_coca_set():\n",
        "  value = int(value)\n",
        "  if(context in counter_without_words):\n",
        "      counter_without_words[context]+= value\n",
        "  else:\n",
        "      counter_without_words[context]= value\n",
        "\n",
        "  if((word, context) in counter_with_words):\n",
        "      counter_with_words[(word,context)]+= value\n",
        "  else:\n",
        "      counter_with_words[(word,context)]= value\n",
        "\n",
        "  dictionary[word]+=value\n",
        "  all_words_counter+=value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6h_rP4CO9e0P"
      },
      "outputs": [],
      "source": [
        "# calculating probabilities of words in context\n",
        "for key in counter_with_words.keys():\n",
        "  if key[1] not in prob_dict:\n",
        "     prob_dict[key[1]] = dict()\n",
        "\n",
        "  prob_dict[key[1]][key[0]] = counter_with_words[key]/counter_without_words[key[1]]\n",
        "\n",
        "  min_prob = min(min_prob, prob_dict[key[1]][key[0]])\n",
        "min_prob = log10(min_prob) # log not to work with *, but with +-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### By this moment, the model is trained and ready to be used. Next are the function to use the model for correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "F6q9dFgk-_4x"
      },
      "outputs": [],
      "source": [
        "def get_prob(context, word):\n",
        "  if context not in prob_dict and word in dictionary:\n",
        "     return log10(dictionary[word] / all_words_counter)\n",
        "  \n",
        "  if context not in prob_dict or word not in prob_dict[context]:\n",
        "     return log10(0.0001)+min_prob # sum of logs is product of their arguments, that are probabilities. so this value is always < min_prob\n",
        "\n",
        "  return log10(prob_dict[context][word]) # log for better numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "b8i72kdklIPB"
      },
      "outputs": [],
      "source": [
        "def edit_distance(s1,s2, drop_when = 2):\n",
        "  # function to calculate edit distance\n",
        "    if abs(len(s1) - len(s2)) > drop_when: # optimization, since we check edit_distance for word and every word in dict\n",
        "      return drop_when+1\n",
        "\n",
        "    dp = [[0 for j in range(len(s2)+1)] for i in range(len(s1)+1)]\n",
        "    for i in range(1, len(s2)+1):\n",
        "        dp[0][i] = i\n",
        "\n",
        "    for i in range(1, len(s1)+1):\n",
        "        dp[i][0] = i\n",
        "\n",
        "    for i in range(1, len(s1) + 1):\n",
        "        for j in range(1, len(s2) + 1):\n",
        "            dp[i][j] = min(min(dp[i-1][j],dp[i][j-1])+1, dp[i-1][j-1] + (1 if s1[i-1] != s2[j-1] else 0))\n",
        "\n",
        "    return dp[len(s1)][len(s2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HA-oggxwkQmx"
      },
      "outputs": [],
      "source": [
        "def get_corrected_word(context, word, next_word):\n",
        "    # main function\n",
        "    if context not in prob_dict:\n",
        "        return word\n",
        "\n",
        "    corrected_word = word\n",
        "\n",
        "    max_prob = get_prob(context,word)+get_prob(word, next_word) # sum since we deal with logs, without log it is the product\n",
        "    for cand in dictionary: #iterating over all words in dictionary\n",
        "        if (edit_distance(cand, word) <= 2): # check only candidates with edit distance 2, otherwise too far\n",
        "            if get_prob(context, cand) + get_prob(cand, next_word) > max_prob: # left is probability of word in context of 2 word. product in log gives sum\n",
        "                # choose candidate with bigger probability\n",
        "                max_prob = get_prob(context, cand) + get_prob(cand, next_word)\n",
        "                corrected_word = cand\n",
        "\n",
        "    return corrected_word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "### Justifications on training and algorithms itself\n",
        "- I decided to use 2-grams, so only 1 word left and right is taken into account. Using larger n is not a big deal, but small n takes much less amount of data to train + less time + less space.\n",
        "- The datasets are those that were given with the task: bigrams.txt and coca_all_links.txt.\n",
        "- When working with probabilities, I used logarythms to avoid products of small float numbers. The technique is taken from one of the sources I was inspired by when writing.\n",
        "- In general, training is the computation of probability of every word in dictionary to appear in different contexts.\n",
        "- When model is asked to correct the word, it finds the new word (by iterating the whole dictionary) that is close (in terms of [edit distance](https://www.geeksforgeeks.org/edit-distance-dp-5/) that should be <=2) and has the highest probability of being the correct word given context (word before and after)\n",
        "- Iterating over the whole dictionary to find close words is much better than iterating over all corrections of the word, since there are ~65k words in dictionary, while the word of even 6 symbols has >100k different misspelings.\n",
        "- In case when context is not given or unknown, model gives higher probability to word that appears more frequently in the corpus.\n",
        "- If the word to check probability is not known, model gives very small probability to such word to give a sense that this combination is nonsense.\n",
        "### Nuances in testing\n",
        "- The fivegrams.txt is used for testing: given 5 words, I introduce error in the second and fourth words and use them and their contexts as test cases.\n",
        "- When generating test cases, there is 30% probability that the word will not be changed. It is done to check whether any algorithm will replace a correct word by incorrect (for example, \"a\" and \"the\" can be easily substituted by each other).\n",
        "- As possible errors, I used the same code as in norvig solution: deletes, transposes, replaces and inserts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3yWbnKSgrCT3"
      },
      "outputs": [],
      "source": [
        "# from norvig, used to generate error\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return [e2 for e1 in edits1(word) for e2 in edits1(e1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nuP67BzLrRjT"
      },
      "outputs": [],
      "source": [
        "def generate_error(word, p_no_error = 0.3):\n",
        "  # Here we generate spell error for given word.\n",
        "  # with chance p_no_error we do not make any error\n",
        "  # in other case, we generate error up to 2 edits\n",
        "  if random.random() > 1 - p_no_error:\n",
        "    return word\n",
        "  possible_errors = edits2(word)\n",
        "  return random.choice(possible_errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "R_d27kji4ZAh"
      },
      "outputs": [],
      "source": [
        "# here we generate test set\n",
        "test_set_correct = []\n",
        "test_set_errors = []\n",
        "test_size = 50\n",
        "\n",
        "for i in range(test_size):\n",
        "  n_text, a, b, c, d, e = linecache.getline(\"./data/fivegrams.txt\", random.randrange(0, 1040620)).split()\n",
        "  test_set_correct.append([a,b,c,d,e])\n",
        "  b,d = generate_error(b), generate_error(c)\n",
        "  test_set_errors.append([a,b,c,d,e])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test of bigram model on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOeiyTKzk-BY",
        "outputId": "4fd0ef4b-bfbf-409c-eb63-620ea6bf686b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:46<00:00,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy of bigrams is  0.45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "total = test_size * 2\n",
        "correct = 0\n",
        "for i in tqdm(range(len(test_set_errors))):\n",
        "  a,b,c,d,e = test_set_errors[i]\n",
        "  correction1 = get_corrected_word(a,b,c)\n",
        "  if correction1 == test_set_correct[i][1]:\n",
        "    correct +=1\n",
        "\n",
        "  correction2 = get_corrected_word(c,d,e)\n",
        "  if correction2 == test_set_correct[i][3]:\n",
        "    correct +=1\n",
        "print(\"\\nAccuracy of bigrams is \", correct/total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yP8PI6Pt7Mh"
      },
      "source": [
        "### Test of norvig's solution on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open(\"./data/big.txt\").read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())):\n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word):\n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Txcy0DemxOql",
        "outputId": "51a1a134-a3e0-4bd5-f7f6-3201e59bb421"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:01<00:00, 27.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy of Norvig's solution is  0.32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "total = test_size * 2\n",
        "correct = 0\n",
        "for i in tqdm(range(len(test_set_errors))):\n",
        "  a,b,c,d,e = test_set_errors[i]\n",
        "  correction1 = correction(b)\n",
        "  if correction1 == test_set_correct[i][1]:\n",
        "    correct +=1\n",
        "\n",
        "  correction2 = correction(d)\n",
        "  if correction2 == test_set_correct[i][3]:\n",
        "    correct +=1\n",
        "print(\"\\nAccuracy of Norvig's solution is \", correct/total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guniFCieyWhi"
      },
      "source": [
        "## Comparison results\n",
        "Norvig's solution give 32% of accuracy.\\\n",
        "Bigrams give 45% of accuracy, that is the improvement."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
