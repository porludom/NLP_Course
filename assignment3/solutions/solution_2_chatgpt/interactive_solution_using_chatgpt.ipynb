{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second solution to assignment 2 with the use of ChatGPT\n",
    "First, we start with the step by step solution.\\\n",
    "At the end, I will present the way to use the functionality without any effort with one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import string\n",
    "import shutil\n",
    "from typing import List\n",
    "from datasets import load_dataset\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using ChatGPT, I use service Proxy API that provided additional endpoint by the link https://api.proxyapi.ru/openai/v1. It allows us to pay for tokens using rubles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_api_key = \"hehehehehehehehehehehehe\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=proxy_api_key,\n",
    "    base_url=\"https://api.proxyapi.ru/openai/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка разных наборов данных\n",
    "\n",
    "with open(\"../../data/train.jsonl\", \"r\") as f:\n",
    "    train_data = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "with open(\"../../data/dev.jsonl\", \"r\") as f:\n",
    "    dev_data = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "with open(\"../../data/test.jsonl\", \"r\") as f:\n",
    "    test_data = [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we send every text from test set to ChatGPT with the task of finding all nested named entitities. The list of them is given in the context. The example of the solution is given too\\\n",
    "The output of the model is the string representation of list of lists, where each sublist is one entity of format [\"text of the entity\", \"entity type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [04:10<00:00,  3.85s/it]\n"
     ]
    }
   ],
   "source": [
    "replies = [] \n",
    "for sample in tqdm(test_data):\n",
    "    text = sample[\"senences\"] # extracting text\n",
    "\n",
    "    # asking for the entities\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=[{\"role\": \"system\", \"content\": \"Пользователь даст текст. Тебе нужно найти все сущности в нем. Список сущностей на английском: AGE, AWARD, CITY, COUNTRY, CRIME, DATE, DISEASE, DISTRICT, EVENT, FACILITY, FAMILY, IDEOLOGY, LANGUAGE, LAW, LOCATION, MONEY, NATIONALITY, NUMBER, ORDINAL, ORGANIZATION, PENALTY, PERCENT, PERSON, PRODUCT, PROFESSION, RELIGION, STATE_OR_PROVINCE, TIME, WORK_OF_ART. Тебе нужно ответить без лишних комментариев в следующей форме: [[\\\"Газпром\\\", \\\"ORGANIZATION\\\"], [\\\"Роман\\\", \\\"PERSON\\\"]]. Например, для текста \\\"Я работаю учителем\\\", тебе нужно ответить [[\\\"учителем\\\", \\\"PROFESSION\\\"]], потому что в тексте есть упоминание профессии. Пиши как было в тексте, не меняй падеж и прочее!\"},\\\n",
    "                                     {\"role\" : \"system\", \"content\" : \"Еще пример выполнения задания. Часть правильного ответа к некоторому тексту: \\\"[[\\\"23-летнюю\\\", \\\"AGE\\\"], [\\\"Синьцзян-Уйгурского автономного района\\\", \\\"DISTRICT\\\"], [\\\"Ма Ай Лунь\\\", \\\"PERSON\\\"], [\\\"Китаянка\\\", \\\"NATIONALITY\\\"], [\\\"iPhone\\\", \\\"PRODUCT\\\"], [\\\"китаянки\\\", \\\"NATIONALITY\\\"], [\\\"убило током\\\", \\\"EVENT\\\"], [\\\"убило током\\\", \\\"EVENT\\\"], [\\\"погибла\\\", \\\"EVENT\\\"], [\\\"полиция\\\", \\\"ORGANIZATION\\\"], [\\\"гибель\\\", \\\"EVENT\\\"], [\\\"правоохранительные органы\\\", \\\"ORGANIZATION\\\"], [\\\"гибели\\\", \\\"EVENT\\\"]]\\\". Обрати внимание, что считается сущностью EVENT.\"},\n",
    "                                     {\"role\": \"user\", \"content\": text}\\\n",
    "                                        ])\n",
    "    \n",
    "    # extracting the text answer out of answer object\n",
    "    reply = chat_completion.choices[0].message.content\n",
    "    replies.append(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[\"Бирмингем Сити\", \"ORGANIZATION\"], [\"Карсон Ёнг\", \"PERSON\"], [\"Гонконга\", \"CITY\"], [\"55 миллионов фунтов стерлингов\", \"MONEY\"], [\"2001\", \"DATE\"], [\"2007 годы\", \"DATE\"], [\"Карсон Ёнг\", \"PERSON\"], [\"Бирмингема\", \"ORGANIZATION\"], [\"2009 году\", \"DATE\"], [\"81,5 миллионов фунтов стерлингов\", \"MONEY\"]]'"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is the example of extracted entities for the first text\n",
    "replies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [s[\"id\"] for s in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to do is to transform the output to the desired format the system wants and to save the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.jsonl\", \"w\") as f:\n",
    "    for i, reply in enumerate(replies):\n",
    "\n",
    "        # try except for debugging purposes\n",
    "        try:\n",
    "            # The output of the model is string with the list we need. This function transforms the string of list of lists TO list of lists\n",
    "            list_of_entities = ast.literal_eval(reply.replace(\"\\\"\\\"\", \"\\\"\"))\n",
    "        except:\n",
    "            print(i)\n",
    "            break\n",
    "        \n",
    "        text = test_data[i][\"senences\"].lower()\n",
    "        entities_in_good_format = [] # This is the list of entities in the desired format [1,5, \"PERSON\"] instead of [\"Roman\", \"PERSON\"]\n",
    "        # in this loop we replace entities by their locations in the text\n",
    "        for entity in list_of_entities:\n",
    "            word = entity[0]\n",
    "            entity_type = entity[1]\n",
    "\n",
    "            start = text.find(word.lower())\n",
    "            if start == -1: # did not found such entity. happens sometimes, just skip\n",
    "                continue\n",
    "            end = start + len(word.lower()) - 1\n",
    "\n",
    "            entities_in_good_format.append([start, end, entity_type])\n",
    "            \n",
    "        id = ids[i]\n",
    "        d = {\"id\" : id, \"ners\" : entities_in_good_format}\n",
    "        f.write(str(d).replace(\"'\", \"\\\"\"))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: test.jsonl (164 bytes security) (deflated 77%)\n"
     ]
    }
   ],
   "source": [
    "# .jsonl to .zip\n",
    "!zip test test.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output/test.jsonl'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.move(\"./test.jsonl\", \"./output/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output/test.zip'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.move(\"./test.zip\", \"./output/test.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of usage for any text easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above was the step by step solution like the stream of code. Here is the example how to apply the same functionality on your text using the predefined function you can find in the file NNER_model\\\n",
    "It requires only the text to process and the token for the Proxy API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNER_model import extract_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18, 22, 'PERSON']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_entities(\"Hello. My name is Roman\", proxy_api_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
